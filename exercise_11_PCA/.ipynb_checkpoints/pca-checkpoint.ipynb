{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11: Dimensionality Reduction\n",
    "In this exercise, we will implement and see the workings of a dimensionality reduction technique: Prinical Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good to import few packages\n",
    "%matplotlib notebook\n",
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Toy Dataset\n",
    "Let see the PCA results on a toy iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data = iris['data'].astype(np.float32) \n",
    "labels = iris['target'] \n",
    "cls_names = iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first visualize the trends of different features together. One can see that one class is well separated from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "count = 1\n",
    "colors = np.array([[0.85, 0.85, 0], [0, 0.5, 0], [0.25, 0.25, 1]])\n",
    "for i in range(3):\n",
    "    for j in range(i+1,4):\n",
    "        plt.subplot(2,3,count)\n",
    "        for ind,name in enumerate(cls_names):\n",
    "            filtered_class = labels==ind\n",
    "            plt.scatter(data[filtered_class,i],data[filtered_class,j],c=colors[ind,None],label=name)\n",
    "        plt.xlabel(f'feature_{i}')\n",
    "        plt.ylabel(f'feature_{j}')\n",
    "        plt.legend()\n",
    "        count +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA\n",
    "In the Iris dataset, we have 4 features per data point. Let's now try to reduce the dimensionality from $D=4$ to $d=2$ using PCA. \n",
    "As seen in class, for a dataset $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$ and for a 1D projection ${\\bf w}_{(1)}$, PCA solves the following optimization problem\n",
    "   \\begin{align}\n",
    "    \\underset{\\mathbf{w}_{(1)}}{\\operatorname{max}} \\mathbf{w}_{(1)}^T\\mathbf{C}\\mathbf{w}_{(1)}\\\\\n",
    "    s.t. ~~~~~ \\mathbf{w}_{(1)}^T\\mathbf{w}_{(1)} = 1\n",
    "   \\end{align}\n",
    "   \n",
    " where $\\mathbf{C}$ is the data covariance matrix\n",
    "     \\begin{align}\n",
    "         \\mathbf{C} &= \\frac{1}{N}\\sum_{i=0}^{N-1}(\\mathbf{x}_i-\\mathbf{\\bar{x}})(\\mathbf{x}_i-\\mathbf{\\bar{x}})^T\\\\\n",
    "         \\mathbf{\\bar{x}} &= \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbf{x}_i\n",
    "     \\end{align}\n",
    "     \n",
    " and $\\mathbf{w}_{(1)}\\in \\mathbb{R}^{D}$ is the projection vector we are looking for, $\\mathbf{x}\\in \\mathbb{R}^{D\\times 1}$ is one data sample, and $\\mathbf{\\bar{x}} = \\tfrac{1}{N}\\sum_{i=0}^{N-1}\\mathbf{x}_i \\in \\mathbb{R}^{D\\times 1}$ is the mean of the data.\n",
    " \n",
    " The solution to this problem consists in finding the eigenvector of data covariance matrix $\\mathbf{C}$ with the largest eigenvalue. To project to $d>1$ dimensions, one take the $d(<D)$ eigenvectors with largest eigenvalues and aggregates them into a matrix $\\mathbf{W} = [\\mathbf{w}_{(1)}, \\mathbf{w}_{(2)}, ..., \\mathbf{w}_{(d)} ]$. Hence, $\\mathbf{W}$ is a matrix of $d$ eigenvectors each being $D$-dimensional. \n",
    " \n",
    "Once $\\mathbf{W}$ has been found, we can project our original data $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$ to $\\mathbf{Y}\\in \\mathbb{R}^{N\\times d}$, using the centered data $\\tilde{\\mathbf{X}}\\in R^{N\\times D}$,\n",
    "    \\begin{align}\n",
    "        \\mathbf{Y} &= \\mathbf{\\tilde{X}}\\mathbf{W} \\\\\n",
    "        \\tilde{\\mathbf{x}}_i &= \\mathbf{x}_i-\\mathbf{\\bar{x}} ~~~~ \\text{for } 0 \\leq i \\leq N-1\n",
    "    \\end{align}\n",
    " \n",
    "Finally, to understand how much of the variance is explained by our $d$ eigenvectors, we compute the percentage of the variance explained as \n",
    "    \\begin{align}\n",
    "        \\mathbf{exvar} = \\frac{\\sum_{i=0}^{d-1}\\lambda_i}{\\sum_{i=0}^{D-1}\\lambda_i}\n",
    "    \\end{align}\n",
    "where $\\lambda_i$ is the ith largest eigenvalue. For different applications, one would like to choose $d$ such that the explained variance is greater than a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are asked to code the ```PCA``` that implements the above procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "    X: NxD matrix representing our data\n",
    "    d: Number of principal components to be used to reduce dimensionality\n",
    "    \n",
    "Output:\n",
    "    mean_data: 1xD representing the mean of the input data\n",
    "    W: Dxd matrix representing the principal components\n",
    "    eg: d values representing the variance corresponding to the principal components, ie. the eigenvalues\n",
    "    Y: Nxd data projected in the principal components' direction\n",
    "    exvar: explained variance by the principal components in percentage [%]\n",
    "'''\n",
    "def PCA(X, d):\n",
    "    \n",
    "    # Compute the mean of data\n",
    "    mean = ...\n",
    "    # Center the data with the mean\n",
    "    X_tilde = ...\n",
    "    # Create the covariance matrix\n",
    "    C = ...\n",
    "    # Compute the eigenvectors and eigenvalues. Hint: use np.linalg.eigh\n",
    "    eigvals, eigvecs = ...\n",
    "    # Choose the top d eigenvalues and corresponding eigenvectors. Sort the eigenvalues( with corresponding eigenvectors )\n",
    "    # in decreasing order first.\n",
    "    eigvals = ...\n",
    "    eigvecs = ...\n",
    "\n",
    "    # Create matrix W and the corresponding eigen values\n",
    "    W = ...\n",
    "    eg = ...\n",
    "\n",
    "    # project the data using W\n",
    "    Y = ...\n",
    "    \n",
    "    # Compute the explained variance. Note: we want it in percentage\n",
    "    exvar = ...\n",
    "\n",
    "    return mean, W, eg, Y, exvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the above function and visualize the projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "mean, W, eg, Y, exvar = PCA(data, d)\n",
    "print(f'The total variance explained by the first {d} principal components is {exvar} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ind,name in enumerate(cls_names):\n",
    "    filtered_class = labels==ind\n",
    "    plt.scatter(Y[filtered_class,0], Y[filtered_class,1], c=colors[ind,None], label=name)\n",
    "plt.xlabel(f'feature_0')\n",
    "plt.ylabel(f'feature_1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What happens when d=D?  \n",
    "\n",
    "**Q.** What happens when D>>N?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EigenFaces\n",
    "Now, we will use PCA on images of faces. The goal is to represent faces in the dataset as a linear combination of so-called *eigenfaces*, ie. eigenvectors of this dataset of faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces().data\n",
    "print(f'Dimensions of the Face dataset: N={faces.shape[0]}, D={faces.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PCA on this dataset, and try different values of $d$ to see the impact on the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 30\n",
    "mean, W, eg, Y, exvar = PCA(faces, d)\n",
    "print(f'The total variance explained by the first {d} principal components is {exvar} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize\n",
    "Let us see what the mean face and the principal components look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mean.reshape(64,64), cmap='gray')\n",
    "plt.title('Mean Face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 10 principal components\n",
    "plt.figure(figsize=(8,18))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.imshow(W.reshape(64,64,-1)[...,i], cmap='gray')\n",
    "    plt.xlabel(f'Principal Component:{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what these components account for. Vary the slider to change the principal component id and its influence on the mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "line = ax.imshow(mean.reshape(64,64),cmap='gray')\n",
    "\n",
    "def update(pcind = 0,pcweight=0):\n",
    "    img = W.copy()[:,pcind]*pcweight\n",
    "    line.set_data((img+mean).reshape(64,64))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update,pcind=(0,d-1,1),pcweight=(-10,10,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Can you identify what component accounts for what?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reconstruction\n",
    "We can now project one original data sample $\\mathbf{x}_i \\in \\mathbb{R}^{D}$ to a lower-dimensional representation $\\mathbf{y}_i  \\in \\mathbb{R}^{d} $ using $\\mathbf{W} \\in \\mathbb{R}^{D\\times d}$ using the following operation: $$\\mathbf{y}_i = \\mathbf{W}^T (\\mathbf{x}_i - \\bar{\\mathbf{x}})$$\n",
    "\n",
    "From this compressed representation $\\mathbf{y}_i$, we can recover an approximation of the original data $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^{D}$ by using the opposite projection:\n",
    "$$\\hat{\\mathbf{x}}_i = \\bar{\\mathbf{x}} + \\mathbf{W}\\mathbf{y}_i$$\n",
    "\n",
    "\n",
    "Depending on how many dimension $d$ are kept, we will have some loss of information. Here we will see how changing $d$ affects the reconstruction $\\hat{\\mathbf{x}}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different values of d\n",
    "d = 10\n",
    "mean, W, eg, Y, exvar = PCA(faces, d)\n",
    "print(f'The total variance explained by the first {d} principal components is {exvar} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first select a random face from the dataset\n",
    "sample_id = np.random.choice(faces.shape[0],1)[0]\n",
    "sample_face = faces[sample_id,:]\n",
    "# TODO: project this face to its smaller dimension representation\n",
    "proj_face = ...\n",
    "# TODO: undo the projection (by applying W.T), to recover \n",
    "# an approximation of the initial face, from proj_face\n",
    "reconstructed_face = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now visualize the original face, and the one reconstructed from\n",
    "# the projection on the d first eigen vectors\n",
    "plt.figure()\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(sample_face.reshape(64,64),cmap='gray')\n",
    "ax.set_title('Original Image')\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_face.reshape(64,64),cmap='gray')\n",
    "ax.set_title('Reconstructed Image')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
